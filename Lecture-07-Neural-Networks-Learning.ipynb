{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import matplotlib\n",
      "%matplotlib inline\n",
      "matplotlib.rcParams['figure.figsize'] = (8, 6) # set default figure size, 8in by 6in"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Video W5 01: Neural Network Cost Function\n",
      "\n",
      "[YouTube Video Link](https://www.youtube.com/watch?v=18X68kLAfKY&list=PLZ9qNFMHZ-A4rycgrgOYma6zxF4BZGGPW&index=51)\n",
      "\n",
      "We will be looking this week at methods to train a neural network, e.g. ways to find the set of $\\Theta$ parameters that optimize\n",
      "our network performance on a given training set.  We will be looking at multilayer networks, with usually at least 1 layer of so\n",
      "called hidden units, and a final layer of output units.  We will be doing either binary or multi-class classification with\n",
      "our networks.  For binary classification, we would simply have a single unit in the output layer, and the answer we are looking for\n",
      "is is this a positive or a negative case.  When we have $N$ multiple classes, as we already discussed, we can use a network with\n",
      "$N$ units in the output layer, and we will train such that each unit represents a particular classification.\n",
      "\n",
      "**Cost Function**\n",
      "\n",
      "The cost function we need to use for a neural network is a generalization of the cost function we used for logistic regression. \n",
      "Recall that our logistic regression cost function with the regularization term looked like this:\n",
      "\n",
      "$$\n",
      "J(\\theta) = -\\frac{1}{m} \\Big[ \\sum_{i=1}^m  y^{(i)} \\; \\textrm{log} (h_\\theta(x^{(i)})) + (1 - y^{(i)}) \\; \\textrm{log} (1 - h_\\theta(x^{(i)})) \\Big] + \\frac{\\lambda}{2m} \\sum_{j=1}^{n} \\theta_j^2\n",
      "$$\n",
      "\n",
      "The biggest change in notation for a neural network is that we need to sum up over multiple output units (for the most general\n",
      "multi-class case).  Thus, if we have $K$ output units, we sum up their individual costs:\n",
      "\n",
      "$$\n",
      "J(\\theta) = -\\frac{1}{m} \\Big[ \\sum_{i=1}^m  \\sum_{k=1}^{K} y_k^{(i)} \\; \\textrm{log} (h_\\theta(x^{(i)}))_k + (1 - y_k^{(i)}) \\; \\textrm{log} (1 - h_\\theta(x^{(i)}))_k \\Big] + \\frac{\\lambda}{2m} \\sum_{l=1}^{L-1} \\sum_{i=1}^{s_l}\\sum_{j=1}^{s_{l+1}} \\big( \\Theta_{ji}^{(l)} \\big)^2\n",
      "$$\n",
      "\n",
      "Notice also that the regularization term has become more complex.  This is because we need to add in penalities for our cost for all\n",
      "of the $\\Theta$ parameter values in all of the layers of our network."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Video W5 02: Backpropagation Algorithm\n",
      "\n",
      "[YouTube Video Link](https://www.youtube.com/watch?v=SvAEX5taVKk&list=PLZ9qNFMHZ-A4rycgrgOYma6zxF4BZGGPW&index=52)\n",
      "\n",
      "The trick with multi-layer neural networks is calculating the partial derivative or gradient terms in the layers of the network.\n",
      "Because of the layered nature of the network, there is no direct way to calculate the partial derivatives for hidden layers.  We\n",
      "can however estimate these partial derivatives by calculating the delta or differences in our outputs at the output layer from the\n",
      "correct output.  Given these deltas, we can estimate deltas for subsequent earlier layers.  Thus backpropagation works by first\n",
      "doing a feed forward pass to calculate all of the activations for all of the units in all layers, then backpropagating the delta\n",
      "errors, wich can give us an estimate of the partial derivatives of the functions at each layer.  Don't worry too much if you don't\n",
      "follow the logic for how the backpropagation equations have been derived.  For this course, it will be sufficient to understand\n",
      "the given backpropagation equations so that you can implment them in Python code.\n",
      "\n",
      "The delta's for the output layer are computed dirrectly as simply the difference between the activiation of each unit and the\n",
      "correct answer given in our training set $y$.  For the 4 layer example network from the video, the delta's at layer $L = 4$ are\n",
      "given by:\n",
      "\n",
      "$$\n",
      "\\delta_j^{(4)} = a_j^{(4)} - y_j\n",
      "$$\n",
      "\n",
      "As you can see, this is simply the difference between the output and the correct answer for each of the $j$ output units.  Given\n",
      "these delta values for the output layer, we can estimate deltas for the 2 previous layers:\n",
      "\n",
      "$$\n",
      "\\delta_j^{(3)} = (\\Theta^{(3)})^T \\delta^{(4)} \\; .* \\; g'(z^{(3)})\n",
      "$$\n",
      "\n",
      "$$\n",
      "\\delta_j^{(2)} = (\\Theta^{(2)})^T \\delta^{(3)} \\; .* \\; g'(z^{(2)})\n",
      "$$\n",
      "\n",
      "Notice each calculation of a layer uses the delta calculated from the next higher layer.  The $g'(z)$ represents the derivative\n",
      "of the sigmoid function, which can be derived directly using calculus.  In the video the instructor uses a bit of matlab\n",
      "notation in these equations.  The $.*$ means we need to do an element wise multiplication between the left and right terms.\n",
      "The result will be deltas for each of the units in the indicated layer of the network.  These deltas can then be used directly\n",
      "as estimates of the gradient or partial derivatives, and thus can then be used in our optimization methods like gradient descent to search for the best $\\Theta$ parameters for a given network to represent a given training set of data.\n",
      "\n",
      "**Backpropagation Algorithm**\n",
      "\n",
      "Given a training set of $m$ training examples ${ (x^{(1)}, y^{(1)}), \\cdots, (x^{(m)}, y^{(m)})}$ the video next gives\n",
      "pseudo code for the basic backpropagation algorithm.  There are a lot of details here, but for all of the details it is mostly\n",
      "a matter of being comfortable with the notation.  We are using subscripts $i, j$ to denote connections or $\\Theta$ parameters\n",
      "from the $j^{th}$ unit in a previous layer to the $i^{th}$ unit in the next layer.  And we are using $l$ to indicate the layer\n",
      "number in the network.\n",
      "\n",
      "Given this notation, we create a number of matrices (denoted by capital Delta $\\Delta$, that we initially set to 0 and use\n",
      "as accumulators when computing the deltas.  The algorithm given in the video is:\n",
      "\n",
      "Set $\\Delta_{ij}^{(l)} = 0$ (for all $l, i, j$).\n",
      "\n",
      "For $i = 1 \\; \\textrm{to} \\; m$  (we iterate over each of our training examples)\n",
      "\n",
      "- Set $a^{(1)} = x^{(1)}$ and perform forward propagation to compute the activation $a^{(l)}$ for all units in all layers $l = 2, 3, ..., L$.\n",
      "- Using $y^{(i)}$ compute the delta in the output layer $\\delta^{(L)} = a^{(L)} - y^{(i)}$ \n",
      "- Backpropagate and compute the delta values in all previous layers\n",
      "- Accumulate this computed delta for input example $m$ by adding $\\Delta_{ij}^{(l)} := \\Delta_{ij}^{(l)} + a_j^{(l)} \\delta_i^{(l+1)}$\n",
      "\n",
      "Finally we can add in a regularization term for the units that don't represent bias units\n",
      "\n",
      "$$\n",
      "D_{ij}^{(l)} := \\frac{1}{m} \\Delta_{ij}^{(l)} + \\lambda \\Theta_{ij}^{(l)} \\; \\; \\textrm{if} \\; \\; j \\ne 0\n",
      "$$\n",
      "\n",
      "$$\n",
      "D_{ij}^{(l)} := \\frac{1}{m} \\Delta_{ij}^{(l)} \\;\\;\\;\\;\\;\\;\\;\\; \\textrm{if} \\; \\; j = 0\n",
      "$$\n",
      "\n",
      "And these $D$ terms can be used as approximations of the partial derivative gradients we need in order to perform an optimization\n",
      "like gradient descent on our $\\Theta$ parameters of the network:\n",
      "\n",
      "$$\n",
      "\\frac{\\partial}{\\partial \\Theta_{ij}^{(l)}} J(\\Theta) = D_{ij}^{(l)}\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Video W5 03: Backpropagation Intuition\n",
      "\n",
      "[YouTube Video Link](https://www.youtube.com/watch?v=q1bQDyV6lsg&list=PLZ9qNFMHZ-A4rycgrgOYma6zxF4BZGGPW&index=53)\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Video W5 04: Implementation Note: Unrolling Parameters\n",
      "\n",
      "[YouTube Video Link](https://www.youtube.com/watch?v=rcDJhGtXMvk&index=54&list=PLZ9qNFMHZ-A4rycgrgOYma6zxF4BZGGPW)\n",
      "\n",
      "Example of the unrolling/reshaping operations from the video, but in `Python`/`NumPy`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# example of the matrix reshaping in Python/NumPy\n",
      "Theta1 = np.ones( (10, 11) )\n",
      "print Theta1\n",
      "print Theta1.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
        " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
        " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
        " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
        " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
        " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
        " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
        " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
        " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
        " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]]\n",
        "(10, 11)\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Theta2 = 2 * np.ones( (10,11) )\n",
      "Theta3 = 3 * np.ones( (1, 11) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# the matlab/octave notation Theta1(:) basically reshapes the matrix into a column vector, the\n",
      "# equivalent in NumPY is\n",
      "Theta3Col = Theta3.reshape( (Theta3.size,1) )\n",
      "print Theta3Col\n",
      "print Theta3Col.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 3.]\n",
        " [ 3.]\n",
        " [ 3.]\n",
        " [ 3.]\n",
        " [ 3.]\n",
        " [ 3.]\n",
        " [ 3.]\n",
        " [ 3.]\n",
        " [ 3.]\n",
        " [ 3.]\n",
        " [ 3.]]\n",
        "(11, 1)\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# so to create the thetaVec column vector, we can do this\n",
      "thetaVec = np.concatenate((Theta1.reshape( (Theta1.size,1) ), \n",
      "                           Theta2.reshape( (Theta2.size,1) ),\n",
      "                           Theta3.reshape( (Theta3.size,1) ) ))\n",
      "print thetaVec.shape\n",
      "#print thetaVec"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(231, 1)\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# to get back the theta matrices from the column vector, we can do something similar to matlab\n",
      "# get the Theta1 values back to a 10x11 matrix, note we use 0 based indexing in NumPy arrays\n",
      "np.reshape(thetaVec[0:110], (10, 11) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 19,
       "text": [
        "array([[ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
        "       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
        "       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
        "       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
        "       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
        "       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
        "       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
        "       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
        "       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
        "       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.]])"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.reshape(thetaVec[110:220], (10, 11) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 20,
       "text": [
        "array([[ 2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.],\n",
        "       [ 2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.],\n",
        "       [ 2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.],\n",
        "       [ 2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.],\n",
        "       [ 2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.],\n",
        "       [ 2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.],\n",
        "       [ 2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.],\n",
        "       [ 2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.],\n",
        "       [ 2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.],\n",
        "       [ 2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.]])"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.reshape(thetaVec[220:231], (1,11) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "array([[ 3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.]])"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Video W5 05: Gradient Checking\n",
      "\n",
      "[YouTube Video Link](https://www.youtube.com/watch?v=I-X8_EcGYik&index=55&list=PLZ9qNFMHZ-A4rycgrgOYma6zxF4BZGGPW)\n",
      "\n",
      "This is a method that helps to debug any gradient descent or optimization methods for backpropagation.  If you introduce subtle bugs in computing\n",
      "the cost or gradient values that are used in an optimization method, the optimization can appear to be working.  However, you\n",
      "can end up with not truly optimal parameters that you would get if your calculation of cost and gradient were completely 100%\n",
      "correct.  The method shown in this video can be used to check that the result you get after optimization is actually the\n",
      "best one possible, and thus that you are computing the cost and gradients correctly.\n",
      "\n",
      "This method is based on approximating the gradient or partial derivative, using the difference of the function at 2 points\n",
      "that are close together (based on the definition of the derivative of a function at a point).\n",
      "\n",
      "If the approximate method of calculating the partial derivative is close to the computed $D$ values, then probably the\n",
      "implementation is correct.  Where close is defined as being the same to some number of decimal places, for example."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Video W5 06: Random Initialization\n",
      "\n",
      "[YouTube Video Link](https://www.youtube.com/watch?v=NhgB6FLyHJc&list=PLZ9qNFMHZ-A4rycgrgOYma6zxF4BZGGPW&index=56)\n",
      "\n",
      "Unlike before in logistic regression, there is a problem with setting the initial values for theta to 0 when using\n",
      "backpropagation.  All of the activations (and all of the delta values) will be the same for all inputs given an initial\n",
      "theta that is all 0's.  \n",
      "\n",
      "A simple way to get around this is to initialize all of the theta paramters to small random values, around 0.  The\n",
      "eqivalent way to create random Theta1, Theta2, etc. matrices of the right shape in NumPy is:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "INIT_EPSILON = 0.01\n",
      "Theta1 = np.random.uniform(-INIT_EPSILON, INIT_EPSILON, (10, 11) )\n",
      "print Theta1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[-0.00261722 -0.00600611  0.00573024  0.00130869  0.00105163 -0.00456578\n",
        "  -0.0032189   0.00513946  0.00013627  0.00039538 -0.00085863]\n",
        " [ 0.00932014  0.00821389  0.00534317  0.00240986 -0.00265902  0.00208526\n",
        "  -0.00024175 -0.00101121  0.00288429  0.00542123  0.003798  ]\n",
        " [ 0.00771747  0.00876105  0.00279752 -0.00522309 -0.00886504  0.00224588\n",
        "   0.00244877 -0.00010216 -0.00478963  0.00411784 -0.00098597]\n",
        " [ 0.00935483 -0.002087   -0.00763686  0.00477467  0.00770313  0.00070646\n",
        "  -0.00562603 -0.00648892 -0.00645094  0.00528099 -0.00225673]\n",
        " [-0.00376865  0.00256707 -0.00137811  0.00151909 -0.00387294  0.00883105\n",
        "  -0.00120893 -0.00878634  0.00244463 -0.00197113  0.00259444]\n",
        " [-0.00641708  0.00802445  0.0073901  -0.00909755 -0.00995254 -0.00444705\n",
        "   0.00934983 -0.00752588 -0.00260093  0.00879952 -0.00430645]\n",
        " [-0.00910386 -0.00524525 -0.00799318  0.00085316 -0.00877818 -0.00132836\n",
        "   0.00400942  0.00067876  0.00718483 -0.00219636  0.0078357 ]\n",
        " [ 0.00955494  0.00341423  0.00463091  0.00120389 -0.00494036  0.00918936\n",
        "  -0.00027687  0.0067944   0.00218271 -0.00158421 -0.00021698]\n",
        " [-0.00869498  0.00708747 -0.00534843 -0.00299986 -0.00172947  0.00991317\n",
        "  -0.00098442  0.00954263 -0.0074745   0.0066034   0.00870598]\n",
        " [-0.00575762 -0.00705709 -0.00712944  0.00342453  0.00176233 -0.00046118\n",
        "   0.00222259  0.00808433  0.00846562 -0.00425456  0.00459659]]\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Video W5 07: Putting it Together\n",
      "\n",
      "[YouTube Video Link](https://www.youtube.com/watch?v=T7-ZsYlFH4M&list=PLZ9qNFMHZ-A4rycgrgOYma6zxF4BZGGPW&index=57)\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Video W5 08: Autonomous Driving\n",
      "\n",
      "[YouTube Video Link](https://www.youtube.com/watch?v=WkmplH50K1k&index=58&list=PLZ9qNFMHZ-A4rycgrgOYma6zxF4BZGGPW)\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
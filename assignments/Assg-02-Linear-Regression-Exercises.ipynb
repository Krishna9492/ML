{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10, 8) # set default figure size, 8in by 6in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 02: Implementing Linear Regression\n",
    "\n",
    "**Due Date:** Friday 9/30/2015 (by midnight)\n",
    "\n",
    "\n",
    "## Introduction \n",
    "\n",
    "In this exercise, you will implement linear regression and get to see it work\n",
    "on data. Before starting on this programming exercise, we strongly recom-\n",
    "mend watching the video lectures and completing the review quizes for\n",
    "the class.\n",
    "\n",
    "In this assignment, you need to fill out the cells with the required\n",
    "code and tasks as described.  You should submit your whole completed\n",
    "`.ipynb` notebook to the appropriate eCollege dropbox once you are\n",
    "finished.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression with One Variable\n",
    "\n",
    "In this part of this exercise, you will implement linear regression with one\n",
    "variable to predict profits for a food truck. Suppose you are the CEO of a\n",
    "restaurant franchise and are considering different cities for opening a new\n",
    "outlet. The chain already has trucks in various cities and you have data for\n",
    "profits and populations from the cities.\n",
    "You would like to use this data to help you select which city to expand\n",
    "to next.\n",
    "\n",
    "### Task 1: Plotting the data\n",
    "The data for this assignment is in a file called `assg-02-data.csv` in our course `data` directory.  In the next cell, load \n",
    "this data into a `pandas` dataframe called `data`.\n",
    "Then get the first column into a `NumPy` array called `x` and the second column into a `NumPy` array called\n",
    "'y'.  Set a variable called `m` to be equal to the number of training examples in the data you load from the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('data/assg-02-data.csv')\n",
    "x = data.population.as_matrix()\n",
    "y = data.profit.as_matrix()\n",
    "m = y.size\n",
    "print m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data in `x` (the inputs) represents the Population of a city in 10,000s of people.  The `y` represents the profit made for\n",
    "each city in the training data, in \\$10,000s of dollars.  Plot the profits made on the y axis as a function of the population size on the x axis\n",
    "in the next cell.  Your final figure should look exactly like this (don't forget to label your axis, and set a title for the\n",
    "figure):\n",
    "\n",
    "<img src=\"files/figures/assg-02-plot-1.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create your plot here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Gradient Descent\n",
    "\n",
    "In this part, you will fit the linear regression parameters $\\theta$ to our dataset by hand using gradient\n",
    "descent.  Recall that the objective of linear regression is to minimize the cost function:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} \\big( h_{\\theta}(x^{(i)}) - y^{(i)} \\big)^2\n",
    "$$\n",
    "\n",
    "where the hypothesis $h_{\\theta}(x)$ is given by the linear model:\n",
    "\n",
    "$$\n",
    "h_{\\theta}(x) = \\theta^Tx = \\theta_0 + \\theta_1x_1\n",
    "$$\n",
    "\n",
    "Recall that the parameters of your model are the $\\theta_j$ values.  These are the values that you\n",
    "will adjust in order to minimize the cost function $J(\\theta)$.  One way to do this as we discussed\n",
    "is to use the batch gradient descent algorithm.  In batch gradient descent, each iteration performs\n",
    "the update:\n",
    "\n",
    "$$\n",
    "\\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} \\big( h_{\\theta}(x^{(i)}) - y^{(i)} \\big) x_j^{(i)}\n",
    "$$\n",
    "\n",
    "(where we simultaneously update $\\theta_j$ for all j)\n",
    "\n",
    "With each step of gradient descent, your parameters $\\theta_j$ come closer to the optimal\n",
    "values that will achieve the lowest cost $J(\\theta)$.\n",
    "\n",
    "**Implementation**\n",
    "\n",
    "In the next cell, we have already set up the data for linear regression (using linear algebra\n",
    "operations to perform the calculations) for you.  In the following cell, we add another dimension to our\n",
    "data to accommodate the $\\theta_0$ intercept term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97\n",
      "(2, 97)\n"
     ]
    }
   ],
   "source": [
    "# the raw data\n",
    "data = pd.read_csv('data/assg-02-data.csv')\n",
    "x = data.population.as_matrix() # the raw inputs\n",
    "y = data.profit.as_matrix() # the correct outputs\n",
    "m = y.size\n",
    "\n",
    "# X needs to be a 2xm shaped set of data, where the first value in each column\n",
    "# is a 1.0, and the next value in each column is our raw inputs\n",
    "X = np.ones( (2, m) )\n",
    "X[1,:] = x # the second row contains the raw inputs\n",
    "\n",
    "print m\n",
    "print X.shape\n",
    "#print X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computing the cost $J(\\theta)$**\n",
    "\n",
    "As you perform gradient descent to minimize the cost function $J(\\theta)$, it is helpful to monitor\n",
    "the convergence by computing the cost.  In this section, you will implement a function to calculate\n",
    "$J(\\theta)$ so you can check the convergence of your gradient descent implementation.\n",
    "\n",
    "Your next task is to complete the code in the following cell, which is a function that computes\n",
    "the cost function $J(\\theta)$.  As you are implementing this function, rememebr that the variables\n",
    "`X` and `y` are not scalar values, but a $97 \\times 2$ matrix and a $97$ item vector whose rows\n",
    "represent examples from the training set respectively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_cost(X, y, theta):\n",
    "    \"\"\"Compute the cost function for linear regression.  \n",
    "    \n",
    "    Given a set of inputs X (we assume that the first column has been \n",
    "    initialized to 1's for the theta_0 parameter), and the correct \n",
    "    outputs for these inputs y, calculate the hypothesized outputs \n",
    "    for a given set of parameters theta.  Then we compute the sum of\n",
    "    the squared differences (and divide the final result by 2*m), \n",
    "    which gives us the cost.\n",
    "    \n",
    "    Args\n",
    "    ----\n",
    "    X (numpy mxn array) - The example inputs, first column is expected\n",
    "       to be all 1's.\n",
    "    y (numpy m size array) - A vector of the correct outputs of length m\n",
    "    theta (numpy nx1 array) - An array of the set of theta parameters\n",
    "       to evaluate\n",
    "       \n",
    "    Returns\n",
    "    -------\n",
    "    J (float) - The sum squared difference cost function for the given\n",
    "       theta parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    # determine the number of training examples from the size of the correct outputs\n",
    "    m = len(y)\n",
    "    \n",
    "    # You need to return the following variable correctly\n",
    "    J = 0.0\n",
    "    \n",
    "    # ===== Your Code Here ======\n",
    "    # Instructions: Compute the cost of a particular choice of theta\n",
    "    # and return the resulting cost J\n",
    "    # HINT: you should be able to use the transpose of theta and the np.dot() \n",
    "    # function to multiple theta.T and X, to get a set of hypothesis.  And you\n",
    "    # should then be able to use numpy operations to compute the squared difference\n",
    "    # without the need for any loops\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have completed the function, the next step is to test it.  Here are some examples of the correct\n",
    "cost you should get, for some different values of the `theta` parameters.\n",
    "\n",
    "```python\n",
    "theta = np.zeros( (2, 1) )\n",
    "print compute_cost(X, y, theta)\n",
    ">>> 32.0727338775\n",
    "\n",
    "theta = np.array([[1.0],\n",
    "                  [1.0]])\n",
    "print compute_cost(X, y, theta)\n",
    ">>> 10.2665204914\n",
    "\n",
    "theta = np.array([[2.0],\n",
    "                  [2.0]])\n",
    "print compute_cost(X, y, theta)\n",
    ">>> 87.1838499274\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theta = np.zeros( (2, 1) )\n",
    "print compute_cost(X, y, theta)\n",
    "\n",
    "theta = np.array([[1.0],\n",
    "                  [1.0]])\n",
    "print compute_cost(X, y, theta)\n",
    "\n",
    "theta = np.array([[2.0],\n",
    "                  [2.0]])\n",
    "print compute_cost(X, y, theta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Descent**\n",
    "\n",
    "Next you will implement the actual gradient descent iterative algorithm in the next cell.  The\n",
    "loop structure for gradient descent has been written for you, and you only need to supply the \n",
    "updates to $\\theta$ within each iteration.\n",
    "\n",
    "As you program, make sure you understand what you are trying to optimize and what is being updated.\n",
    "Keep in mind that the cost $J(\\theta)$ is parameterized by the vector `theta`, not `X` and `y`.\n",
    "That is, we minimize the value of $J(\\theta)$ by changing the values of the vector `theta`, not\n",
    "by changing `X` or `y`.  Refer to the equations from this assignment notebook and our lecture\n",
    "notebooks if you are uncertain.\n",
    "\n",
    "A good way to verify that gradient descent is working correctly is to look at the value of $J(\\theta)$\n",
    "and check that it is decreasign with each step.  The starter code for your gradient descent function\n",
    "below calls `compute_cost` on every iteration and prints the cost.  Assuming you have implemented\n",
    "gradient descent and compute cost correctly, your value of $J(\\theta)$ should never increase, and\n",
    "should converge to a steady value by the end of the algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, theta, alpha, num_iters):\n",
    "    \"\"\"Perform gradient descent iterative algorithm.\n",
    "    \n",
    "    Given a set of inputs X (we assume that the first column has been \n",
    "    initialized to 1's for the theta_0 parameter), and the correct \n",
    "    outputs for these inputs y, and an inital set of theta parameters,\n",
    "    perform iterative gradient descent.  We successively modify the\n",
    "    starting theta parameters in small steps, which should takes us\n",
    "    towards the minimum of the cost function we are exploring.  alpha\n",
    "    is our learning rate parameter, and currently this implementation is\n",
    "    hardcoded to perform exactly num_iters iterations (instead of being\n",
    "    adaptive).\n",
    "    \n",
    "    Args\n",
    "    ----\n",
    "    X (numpy mxn array) - The example inputs, first column is expected\n",
    "       to be all 1's.\n",
    "    y (numpy m array) - A vector of the correct outputs of length m\n",
    "    theta (numpy nx1 array) - An array of the set of theta parameters\n",
    "       to evaluate\n",
    "    alpha (float) - The learning rate to use for iterative gradient\n",
    "       descent\n",
    "    num_iters (int) - The number of gradient descent iterations to perform\n",
    "       \n",
    "    Returns\n",
    "    -------\n",
    "    theta (numpy nx1 array) - The final theta parameters discovered after\n",
    "        out gradient descent.\n",
    "    J_history (numpy num_itersx1 array) - A history of the calculated\n",
    "        cost for each iteration of our descent.\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize some useful values\n",
    "    m = len(y) # number of training examples\n",
    "    J_history = np.zeros( (num_iters, 1) ) # an array of the history of our J cost function\n",
    "    J_history[0] = compute_cost(X, y, theta) # the initial cost\n",
    "    \n",
    "    # perform num_iters iterations of the gradient descent\n",
    "    for iter in range(1, num_iters):\n",
    "        \n",
    "        # ==== Your Code Here ====\n",
    "        # Instructions: Perform a single gradient step on the parameter vector\n",
    "        #    theta.  \n",
    "        # HINT: While debugging, it can be useful to print out the\n",
    "        #    values of the cost function (compute_cost) and gradient here.\n",
    "        # HINT: remember that you must not update theta_0 and then compute\n",
    "        #   the update for theta_1 withe the different theta.  You first need\n",
    "        #   to calcuate all of the new theta parameters using the original,\n",
    "        #   then do a simultaneous update\n",
    "        \n",
    "        # Save the cost J of every iteration in our history so we can return\n",
    "        J_history[iter] = compute_cost(X, y, theta)\n",
    "        \n",
    "    # we return both the final theta parameters and the J_history of the cost\n",
    "    return (theta, J_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# parameters for our algorithms\n",
    "theta = np.zeros( (2, 1) )\n",
    "iterations = 1500;\n",
    "alpha = 0.01\n",
    "\n",
    "theta, J_history = gradient_descent(X, y, theta, alpha, iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have implemented gradient descent correctly, and you run the previous cell to perform gradient\n",
    "descent starting at 0,0 for 1500 iterations, you should get pretty much the following values for\n",
    "the final set of theta parameters and the history of your cost function:\n",
    "\n",
    "```python\n",
    "print theta\n",
    ">>> [[-3.62981201]\n",
    "     [ 1.16631419]]\n",
    "     \n",
    "print J_history\n",
    ">>> [[ 32.07273388]\n",
    "     [  6.73719046]\n",
    "     [  5.93159357]\n",
    "     ..., \n",
    "     [  4.4834581 ]\n",
    "     [  4.48343473]\n",
    "     [  4.48341145]]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print theta\n",
    "print J_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, we replot the data and show your fitted model that was obtained through gradient\n",
    "descent.  This next cell assumes that the `theta` array has model parameters for $\\theta_0, \\theta_1$\n",
    "in them, and will draw a line on the graph representing whatever hypothesis is represented by the\n",
    "`theta` parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot the raw training data\n",
    "plt.plot(x, y, 'rx', label='training data')\n",
    "\n",
    "# plot a representative of our fitted model we learned through gradient\n",
    "# descent\n",
    "x_h = np.linspace(5.0, 22.0, 50)\n",
    "hypothesis = theta[0] + theta[1] * x_h\n",
    "plt.plot(x_h, hypothesis, 'b-', label='linear regression model')\n",
    "\n",
    "plt.xlabel('Population of City in 10,000s')\n",
    "plt.ylabel('Profit in \\$10,000s')\n",
    "plt.title('Total profits as a function of City Population size')\n",
    "plt.legend(loc=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Visualizing $J(\\theta)$ **\n",
    "\n",
    "To understand the cost function $J(\\theta)$ better, you will now plot the cost over a 2-dimensional grid\n",
    "of $\\theta_0$ and $\\theta_1$ values.  You will not need to code anything new for this part, but\n",
    "you should understand how the code you have written already is creating these images.\n",
    "\n",
    "In the next cell, there is code set up to calculate $J(\\theta)$ over a grid ov values using the\n",
    "`compute_cost` function that you wrote.  We use a slightly different method from our lecture\n",
    "notebook where we showed you a contour plot of the cost function.  Here we explicitly use a loop\n",
    "over pairs of $\\theta_0$ and $\\theta_1$ parameters, hopefully to make it as clear as possible how the\n",
    "cost function is shaped and how we obtain it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theta0_vals = np.linspace(-10.0, 10.0, 100)\n",
    "theta1_vals = np.linspace(-1.0, 4.0, 100)\n",
    "J_vals = np.zeros( (theta0_vals.size, theta1_vals.size) )\n",
    "\n",
    "for i in range(theta0_vals.size):\n",
    "    for j in range(theta1_vals.size):\n",
    "        theta_params = np.array([[ theta0_vals[i] ],\n",
    "                          [ theta1_vals[j] ]])\n",
    "        J_vals[i, j] = compute_cost(X, y, theta_params)\n",
    "\n",
    "# because of the way contour plots work, we have to transpose\n",
    "# J_vals before calling surf and coutour, or else the axes will\n",
    "# be flipped\n",
    "J_vals = J_vals.T\n",
    "\n",
    "# do the plots\n",
    "fig = plt.figure(figsize=(16,6))\n",
    "\n",
    "### LEFT subplot\n",
    "# show surface plot of the cost function in 3 dimensions\n",
    "from mpl_toolkits.mplot3d.axes3d import Axes3D\n",
    "t0, t1 = np.meshgrid(theta0_vals, theta1_vals)\n",
    "\n",
    "ax = fig.add_subplot(1,2,1, projection='3d')\n",
    "p = ax.plot_surface(t0, t1, J_vals, rstride=2, cstride=2, cmap=cm.coolwarm)\n",
    "ax.set_xlabel('$\\Theta_0$')\n",
    "ax.set_ylabel('$\\Theta_1$')\n",
    "ax.set_title('(a) Surface')\n",
    "ax.azim = -150\n",
    "\n",
    "### RIGHT subplot\n",
    "# show contour plot, and plot the final theta we achieved using gradient descent\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.contour(t0, t1, J_vals, np.logspace(-2, 3, 20))\n",
    "plt.xlabel('$\\Theta_0$')\n",
    "plt.ylabel('$\\Theta_1$')\n",
    "plt.title('(b) Contour, showing minimum')\n",
    "\n",
    "# display the minimum we achieved with gradient descent on the contour\n",
    "plt.plot(theta[0], theta[1], 'rx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normal Equation Solution**\n",
    "\n",
    "Again there is no coding here, but lets just compare the result you got using gradient descent, to\n",
    "the exact solution to the minimum we obtain using the Normal Equation.  In the previous cell, you should\n",
    "see that you got a gradient descent result after 1500 iterations somewhere around \n",
    "$\\theta_0 = -3.6298, \\theta_1 = 1.1663$.  Lets see what the exact solution gives us for the\n",
    "$\\theta_0, \\theta_1$ parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for the normal equation, we need y in the form of a mx1 column matrix\n",
    "m = y.size\n",
    "y = y.reshape( (m, 1) )\n",
    "\n",
    "# and actually we need X as an mx2 matrix, with ones in the first column.  \n",
    "# this is slightly different from the shape for X we used previously, but\n",
    "# this difference is due to using it before for calculating a set of hypothesis\n",
    "# model values, and here we are using in our normal equation solution\n",
    "X = np.ones( (m, 2) )\n",
    "X[:,1] = x\n",
    "\n",
    "# these lines implement (X^T X)^-1 X^T y\n",
    "# this is the Normal Equation, the values given for Theta are the minimum\n",
    "# fitted parameters for the given data\n",
    "Tmp = np.linalg.inv(np.dot(X.T, X))\n",
    "Theta = np.dot(np.dot(Tmp, X.T), y)\n",
    "print Theta\n",
    "\n",
    "# if we want to see what the actual minimum cost is, we need to put our\n",
    "# X and y matrices back into the shapes we used for our compute_cost function\n",
    "X = X.T\n",
    "y = y.reshape( (m,) )\n",
    "print compute_cost(X, y, Theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have everything working, you should see that the actual minimum solution obtained\n",
    "from the normal equation is close to where we ended up with gradient descent, and that the\n",
    "absolute minimum model we can fit (using our given input parameters) has a cost of\n",
    "4.4769.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

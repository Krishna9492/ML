{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (10, 8) # set default figure size, 8in by 6in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Due: Wednesday 10/31 (by midnight)**\n",
    "\n",
    "Name: \n",
    "\n",
    "CWID:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Linear Regression\n",
    "\n",
    "The following are definitions of data for winning long jump performances at the summer Olympics from 1900 through 1985.  Copy the following data into your notebook and run the cell in order to define these variables:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gold medal winning long jump, in inches\n",
    "long_jump = np.array([282.875, 289, 294.5, 299.25, 281.5, 293.125, 304.75, 300.75, 317.3125, 308, \n",
    "                      298, 308.25, 319.75, 317.75, 350.5, 324.5, 328.5, 336.25, 336.25])\n",
    "y = long_jump # variable y, corresponding to usage in lecture notebooks\n",
    "\n",
    "# corresponding year of olympics\n",
    "year = np.array([1900, 1904, 1908, 1912, 1920, 1924, 1928, 1932, 1936, 1948, 1952, 1956, \n",
    "                 1960, 1964, 1968, 1972, 1976, 1980, 1984])\n",
    "x = year # variable x, corresponding to usage in lecture notebooks\n",
    "\n",
    "# number of training examples\n",
    "m = len(long_jump) \n",
    "\n",
    "# create the X array, wher first row is all 1's, and other rows are the original x inputs\n",
    "# suitable for use in cost and gradient function calculations\n",
    "X = np.ones( (2, m) )\n",
    "X[1:,:] = x.T # the second column contains the raw inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1**: Plot the long jump performance by year (year on the x axis, long jump performance on the y axis).  Use blue circle markers (no lines) to create a scatter plot of the data.  Label your axis for the figure, and add a title to the figure.  Copy and paste your code to create your figure from you notebook into the eCollege text box provided for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# place plot here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2**: Now using the long jump data, perform a linear fit of the data.  For this cell, use the polyfit() and poly1d() functions from the NumPy library to fit a line to the data (we demonstrated using polyfit() functions back in Lecture-03a notebook).  Print out the coeficients you find for the best fit using polyfit().  Replot the figure from the previous task, but add in the line representing the hypothesis/model that polyfit() and poly1d() find to your new figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# perform linear regression using numpy polyfit() and poly1d()\n",
    "\n",
    "# plot figure adding regression line to figure to visualize fitted model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3**: We did not demonstrate using the scikit-learn library for performing linear and logistic regressions,\n",
    "but we did later look in general at the general framework of scikit-learn and how to use it to fit models.\n",
    "Look up the documentation about using scikit-learn to fit linear regression models to data.  Using scikit-learn\n",
    "functions, perform the same linear regression and display the coefficients found of the best fit.  Do they\n",
    "match the same found parameters found using the NumPy library functions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# perform linear regression using scikit-learn functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3**: Perform linear regression using advanced optimization techniques by hand, as we did in our\n",
    "lecture notebooks..  Here are functions suitable for calculating the cost and gradients for\n",
    "simple linear regression of one variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_linear_regression_cost(theta, X, y):\n",
    "    \"\"\"Compute the cost function for linear regression.  \n",
    "    \n",
    "    Given a set of inputs X (we assume that the first column has been \n",
    "    initialized to 1's for the theta_0 parameter), and the correct \n",
    "    outputs for these inputs y, calculate the hypothesized outputs \n",
    "    for a given set of parameters theta.  Then we compute the sum of\n",
    "    the squared differences (and divide the final result by 2*m), \n",
    "    which gives us the cost.\n",
    "    \n",
    "    Args\n",
    "    ----\n",
    "    theta (numpy nx1 array) - An array of the set of theta parameters\n",
    "       to evaluate\n",
    "    X (numpy mxn array) - The example inputs, first column is expected\n",
    "       to be all 1's.\n",
    "    y (numpy m size array) - A vector of the correct outputs of length m\n",
    "       \n",
    "    Returns\n",
    "    -------\n",
    "    J (float) - The sum squared difference cost function for the given\n",
    "       theta parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    # determine the number of training examples from the size of the correct outputs\n",
    "    m = len(y)\n",
    "    \n",
    "    # You need to return the following variable correctly\n",
    "    J = 0.0\n",
    "    \n",
    "    # Compute the cost of a particular choice of theta\n",
    "    # and return the resulting cost J\n",
    "    hypothesis = np.dot(theta.T, X)\n",
    "    J = np.sum( (hypothesis - y)**2.0 ) / (2.0 * m)\n",
    "    \n",
    "    return J\n",
    "\n",
    "\n",
    "def compute_linear_regression_gradients(theta, X, y):\n",
    "    \"\"\"Compute the gradients of the theta parameters for our linear regression\n",
    "    cost function.\n",
    "    \n",
    "    Given a set of inputs X (we assume that the first column has been \n",
    "    initialized to 1's for the theta_0 parameter), and the correct \n",
    "    outputs for these inputs y, calculate the gradient of the cost function\n",
    "    with respect to each of the theta parameters.\n",
    "    \n",
    "    Args\n",
    "    ----\n",
    "    theta (numpy nx1 array) - An array of the set of theta parameters\n",
    "       to evaluate\n",
    "    X (numpy mxn array) - The example inputs, first column is expected\n",
    "       to be all 1's.\n",
    "    y (numpy m size array) - A vector of the correct outputs of length m\n",
    "       \n",
    "    Returns\n",
    "    -------\n",
    "    gradients - A numpy n sized vector of the computed gradients.\n",
    "    \"\"\"\n",
    "\n",
    "    # determine the number of training examples from the size of the correct outputs\n",
    "    # and the number of parameters from the size of theta\n",
    "    m = len(y)\n",
    "    n = len(theta)\n",
    "    \n",
    "    # You need to return the following variable with the correctly calculated\n",
    "    # gradients of theta\n",
    "    gradients = np.zeros(n)\n",
    "\n",
    "    hypothesis = np.dot(theta.T, X)\n",
    "    for j in range(n):\n",
    "        gradients[j] = np.sum((hypothesis - y) * X[j,:]) / m\n",
    "        \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the advanced optimization minimize() method (from the scipy.optimize library), run a minimization of the cost function for the long jump data, using the BFGS optimization method.  Print out the resulting theta parameters that are calculated.  Do they agree with the parameters you calculated using the polyfit() method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# optimize the linear regression fit by hand here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Logistic Regression\n",
    "\n",
    "In assignment 03 we performed a logistic regression classification on a set of data representing student\n",
    "exam scores on 2 exams where students were classified as being admitted or not admitted to the university\n",
    "(based on exam scores and other unknown criteria).  Read the help documentation on using scikit-learn\n",
    "library functions to perform a logistic classification, and fit a classifier to this same data.\n",
    "Create a plot that visualizes the classification achieved by your scikit-learn logistic classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# perform logistic regression using scikit-learn library here\n",
    "\n",
    "# visualize the classification of the data by showing the decision boundary made by the logistic regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Decision Tree\n",
    "\n",
    "You have been hired by a biology professor to create a decision tree based on whether a mushroom is poisonous or\n",
    "not, and have been given the following chart of data:\n",
    "\n",
    "\n",
    "|Colour |Height |Stripes |Texture |Poisonous?\n",
    "|------ |------ |------- |------- |----------\n",
    "|Purple |Tall \t|Yes \t |Rough   |Yes\n",
    "|Purple |Tall \t|Yes \t |Smooth  |Yes\n",
    "|Red \t|Short \t|Yes \t |Hairy   |No\n",
    "|Blue \t|Short \t|No \t |Smooth  |No\n",
    "|Blue \t|Short \t|Yes \t |Hairy   |Yes\n",
    "|Red \t|Tall \t|No \t |Hairy   |No\n",
    "|Blue \t|Tall \t|Yes \t |Smooth  |Yes\n",
    "|Blue \t|Short \t|Yes \t |Smooth  |Yes\n",
    "|Blue \t|Tall \t|No \t |Hairy   |No\n",
    "|Blue \t|Short \t|Yes \t |Rough   |Yes\n",
    "|Red \t|Short \t|No \t |Smooth  |No\n",
    "|Purple |Short \t|No \t |Hairy   |Yes\n",
    "|Red \t|Tall \t|Yes \t |Hairy   |No\n",
    "|Purple |Tall \t|Yes \t |Hairy   |Yes\n",
    "|Purple |Tall \t|No \t |Rough   |No\n",
    "|Purple |Tall \t|No \t |Smooth  |No\n",
    "\n",
    "First of all, put the data into a file and correctly massage or clean it to work well as training data\n",
    "for a decision tree classifier.  You may need to look at converting text into numerical factors, for example.\n",
    "Then using a scikit-learn decision tree classifier, fit a tree to your data.  Show some predictions from\n",
    "you model, for example, given a muchroom that is blue, tall, striped and smooth, does your classifier\n",
    "predict you can eat it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
